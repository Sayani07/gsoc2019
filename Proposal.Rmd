---
title: "Proposal"
author: "Sayani Gupta"
date: "04/04/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Table of Contents**

**Basic Information**
-----------------------

Name: Sayani Gupta

University: Monash University,  Australia

Email: gupta.sayani@gmail.com

Github: https://github.com/Sayani07

Twitter: https://twitter.com/SayaniGupta07

Phone: (+61) 423889998

Postal Address: 2/612 Blackburn Road, Glen Waverly, Victoria 3150

Timezone: AEST (UTC + 11:00)


Abstract
--------
Package gravitas aims to provide methods to operate on time in an automated way, to deconstruct it in many different ways. Deconstructions of time that respect the linear progression of time like days, weeks and months are defined as linear time granularities and those that accommodate for periodicities in time like hour of the day or day of the month are defined as circular granularities or calendar categorizations. 

The package will provide these techniques into the tidy workflow, so that probability distributions can be examined in the range of graphics available in the ggplot2 package.


Motivation
----------

Considerable data is accumulated by sensors today. An example is data measuring energy usage on a fine scale using smart meters. Smart meters are installed on many households in many countries now. Providing tools to explore this type of data is an important activity. Because of the large volume of data, using probability distributions for display is a potentially useful approach. Probability distributions are induced by various aggregations of the data, by temporal components, by spatial region, by type of household.


Project gravitas
----------------

This package will consist of three modules: 

The module **BUILD** will provide the methods to exhaustively construct any granularities. For example, if the data is available at a half hourly scale, this module would create functions to compute time indices that can be constructed in combination with half-hour like half-hour of the day or half-hour of the week. Additionally, it should also be able to provide functions to create granulartities in combination with temporal scales that are one or multiple levels above half-hour like hour, day, week or month. The idea here is to able to provide exahustive set of granularities to figure out periodicities, patterns or anomalies across different granularities that are not obvious and hence aid the process of looking at data from multiple perspectives.

The module **COMPATIBLE** will provide automatic checks on the feasibility of plotting or drawing inference from two granularities together.The idea is to categorize pairs of granularities as either a harmony or clashes, where harmonies are pairs of circular granularities that aid exploratory data analysis. Clashes are pairs that are incompatible with each other for exploratory analysis. 

The module **POINTERS** will provide appropriate data structures to visualize with the grammar of graphics for harmonies. Moreover, it will provide suggestions on the nature of harmonies, which may include number of observations per combination of categories for these harmonies or variation in number of observations across combinations of harmonies. These suggestions will serve as a guide to users who are looking to explore distributions of variables across these harmonies.

Develop a shiny UI to enable user to walk through all these modules seamlessly.

Provide examples of probability visualization of smart meter data collected on Australian households

Document the R package functionality in a vignette

**Proposed Deliverables (during GSoC)**


- (Phase 0) Till 6 May: Pre-GSoC Period

- (Phase 1) 7 May - 26 May: Community Bonding Period

- (Phase 2) 27 May - 23 June: Coding Period 1

- (Phase 3) 24 June - 28 June: Phase 1 Evaluations

- (Phase 4) 29 June -22 July: Coding Period 2

- (Phase 5) 23 July - 26 July: Phase 2 Evaluations

- (Phase 6) 27 July - 18 August: Coding Period 3

- (Phase 7) 19 August - 26 August: Final week of submitting finished product

- (Phase 8) 26 August - 2 September: Final evaluation





The recording module will be responsible for managing the addition of channel lists, set up of recording jobs and saving the recorded streams. The processing module will parse saved samples, associate tags, extract subtitles and convert the video files to MP4 to reduce the file size. The uploading module will upload the processed stream files, and also share sample with other universities if required.
Nephos will be developed, using Python and few other open source projects, to accomplish all the above mentioned tasks with cent-percent reliability and zero failures (unless wrong data is input, which will get logged). Testing and logging will be an integral part of Nephos development and running cycle, respectively.

Background
----------



Motivation
----------



Project gravitas
--------------


Why do I want to work on gravitas?
------------------------------

### Why Nephos?


